{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load imports\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options as ChromeOptions\n",
    "import datetime \n",
    "CHROME_DRIVER = './chromedriver'\n",
    "import time\n",
    "import pandas \n",
    "import os\n",
    "import re\n",
    "\n",
    "def setup_driver(headless=True, driver_type=CHROME_DRIVER) -> webdriver:\n",
    "    options = ChromeOptions()\n",
    "    options.add_argument('--ignore-certificate-errors')\n",
    "    options.add_argument('--ignore-ssl-errors')\n",
    "    if headless:\n",
    "        options.add_argument('-headless')\n",
    "    driver = webdriver.Chrome(executable_path=driver_type, chrome_options=options)\n",
    "    return driver\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_soup(link):\n",
    "    #basic function to parse the soup\n",
    "    driver = setup_driver()\n",
    "    driver.get(link)\n",
    "    time.sleep(8)\n",
    "    soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "    executing_query = \"document.getElementsByClassName('popover-x-button-close icl-CloseButton')[0].click()\"\n",
    "    #in case a pop up appears, it closes it\n",
    "    if soup.find('div',{'id':'popover-form-container'}):\n",
    "        driver.execute_script(executing_query)\n",
    "        soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "    print(driver.current_url)\n",
    "    driver.quit()\n",
    "    return soup\n",
    "\n",
    "\n",
    "def parse_all_pages(page):\n",
    "    #from the first page, it gets the links to all other pages..\n",
    "    page_links = []\n",
    "    blocks = page.find('ul',{'class':'pagination-list'}).find_all('li')\n",
    "    for block in blocks:\n",
    "        if block.find('a', href = True):\n",
    "            incomplete_link = block.find('a')['href']\n",
    "            page_links.append('https://www.indeed.com'+incomplete_link)\n",
    "            \n",
    "    pages = []\n",
    "    #stores the soup of all the pages\n",
    "    pages.append(page)\n",
    "    #appends the soup for the first page before getting into the loop\n",
    "    for link in set(page_links):\n",
    "        pages.append(get_soup(link))\n",
    "        #loops with all the links to other pages and stores their soups..\n",
    "    return pages\n",
    "\n",
    "def get_all_soups(pages):\n",
    "    soups = []\n",
    "    #stores the soups to each job postings, the soup from their specific \n",
    "    links_to_postings = []\n",
    "    #stores all the links to the specific job postings\n",
    "    for page in pages:\n",
    "        method_1 = page.find('div',{'class':'mosaic mosaic-provider-jobcards mosaic-provider-hydrated'})\n",
    "        method_2 = page.find('div',{'class':'jobsearch-SerpJobCard unifiedRow row result clickcard'})\n",
    "        #this takes care of the two different classes used in the html pages\n",
    "        if method_1:\n",
    "            container = method_1\n",
    "            boxes = container.find_all('a', href = True)\n",
    "        elif method_2:\n",
    "            container = method_2\n",
    "            boxes = container.find_all('a', href = True)\n",
    "        else:\n",
    "            return \"we don't have the job postings\"\n",
    "            # quits the method if the links are not found.\n",
    "            \n",
    "        if boxes: #proceeds only if links to different postings exists\n",
    "            for box in boxes:\n",
    "                first = box['href']\n",
    "                if '/rc/clk?jk' in first:\n",
    "                    link = first.replace('/rc/clk?','https://www.indeed.com/viewjob?')\n",
    "                    links_to_postings.append(link)\n",
    "                    #append all the links to the list\n",
    "    for link in set(links_to_postings):\n",
    "        soups.append(get_soup(link))\n",
    "        #get soups for unique job links parsed from different websites\n",
    "    print(f'we got {len(soups)} job postings')\n",
    "    return soups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class job_posting(object):\n",
    "    def __init__(self):\n",
    "        self.job_title = None\n",
    "        self.company = None\n",
    "        self.original_link = None\n",
    "        self.is_full_time = None\n",
    "        self.is_remote = None\n",
    "        self.job_location = None\n",
    "        self.description = None\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "page = get_soup('https://www.indeed.com/jobs?q=%22Software+Engineer%22&l=')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pages = parse_all_pages(page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "soups = get_all_soups(pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_job_title(soup):\n",
    "    title = 'N/A'\n",
    "    title_text = soup.find('h1',{'class':'icl-u-xs-mb--xs icl-u-xs-mt--none jobsearch-JobInfoHeader-title'})\n",
    "    if title_text:\n",
    "        title = title_text.text.title()\n",
    "    return title    \n",
    "    \n",
    "    \n",
    "def parse_link(soup):\n",
    "    link = 'N/A'\n",
    "    link_text = soup.find('div',{'id':'originalJobLinkContainer'})\n",
    "    if link_text:\n",
    "        link = link_text.find('a')['href']\n",
    "    return link\n",
    "        \n",
    "        \n",
    "def parse_company_name(soup):\n",
    "    name = 'unknown'\n",
    "    text = soup.find('div',{'class':'jobsearch-InlineCompanyRating icl-u-xs-mt--xs jobsearch-DesktopStickyContainer-companyrating'})\n",
    "    if text:\n",
    "        name = re.sub('[0-9]','',text.text)\n",
    "    final = name.replace('reviews','').replace(',','').title()\n",
    "    return final\n",
    "\n",
    "\n",
    "def parse_working_time(soup):\n",
    "    soup_text = soup.text.lower()\n",
    "    full_time ='N/A'\n",
    "    if 'full time' in soup_text or 'full-time' in soup_text:\n",
    "        full_time = bool(1)\n",
    "    elif 'part time' in soup_text or 'part-time' in soup_text:\n",
    "        full_time = bool(0)\n",
    "    return full_time\n",
    "    \n",
    "    \n",
    "def parse_job_location(soup):\n",
    "    is_remote = 'N/A'\n",
    "    final = 'N/A'\n",
    "    entire_text = soup.find('div',{'class':'icl-u-xs-mt--xs icl-u-textColor--secondary jobsearch-JobInfoHeader-subtitle jobsearch-DesktopStickyContainer-subtitle'})\n",
    "    if entire_text:\n",
    "        company_name = soup.find('div',{'class':'jobsearch-InlineCompanyRating icl-u-xs-mt--xs jobsearch-DesktopStickyContainer-companyrating'}).text\n",
    "        final = entire_text.text.replace(company_name, '')\n",
    "    if 'remote' in final.lower():\n",
    "        final = final.lower().split('remote')[0].title()\n",
    "        is_remote = True\n",
    "    if final == '':\n",
    "        final = 'Remote'\n",
    "    return final , is_remote\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame()\n",
    "for soup in soups:\n",
    "    job = job_posting()\n",
    "    job.job_title = parse_job_title(soup)\n",
    "    job.original_link = parse_link(soup)\n",
    "    job.company = parse_company_name(soup)\n",
    "    job.is_full_time = parse_working_time(soup)\n",
    "    job.job_location, job.is_remote = parse_job_location(soup)\n",
    "    df = df.append(job.__dict__, ignore_index=True)\n",
    "df = df[['company', 'job_title', 'is_full_time', 'original_link', 'is_remote', 'job_location','description']]\n",
    "todays_date = f'{datetime.datetime.now():%d-%m-%Y-%H-%M}'\n",
    "csv_filename = str(todays_date) +\".csv\"\n",
    "df.to_csv(os.path.expanduser(f'~/Downloads/{csv_filename}'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
